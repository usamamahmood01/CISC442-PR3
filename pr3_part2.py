# -*- coding: utf-8 -*-
"""PR3_Part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UmuGJVgpqntmK9J67SnUhK6IQcsGJizQ
"""

import os
import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import cv2
from PIL import Image
#from google.colab.patches import cv2_imshow
from mpl_toolkits.axes_grid1 import ImageGrid
print(os.getcwd())
'''
idea of coloring the classes taken from
https://learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/
'''
class_map = [
              (0,0,0), # blank
              (49,214,40), # airplane
              (223,105,22), # bicycle
              (133,167,185), # bird
              (208,47,208), # boat
              (203,173,54), # bottle
              (192,86,48), # bus 
              (250,38,22), # car
              (204,224,252), # cat
              (176,129,65), # chair
              (139,37,136), # cow
              (213,93,85), # dining table
              (75,127,95), # dog
              (140,143,13), # horse
              (144,49,113), # motorbike
              (192, 128, 128), # person
              (4,136,0), # potted plant
              (159,85,25), # sheep
              (23,190,207), # sofa
              (128, 192, 0), # train
              (0, 64, 128) # tv/monitor
]

# define image transformation parameters
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.23, 0.225, 0.227))
])

# define model, device and load model
model = torchvision.models.segmentation.fcn_resnet50(pretrained=True)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.eval().to(device)

# import images from directory
images = []
images.append(Image.open(r'/Users/usamamahmood/Developer/CISC442/PR3/Part2/images/bikecarperson.jpg').convert('RGB'))
images.append(Image.open(r'/Users/usamamahmood/Developer/CISC442/PR3/Part2/images/cartrainplane.jpg').convert('RGB'))
images.append(Image.open(r'/Users/usamamahmood/Developer/CISC442/PR3/Part2/images/catdogperson.jpg').convert('RGB'))
images.append(Image.open(r'/Users/usamamahmood/Developer/CISC442/PR3/Part2/images/horsepersoncar.jpg').convert('RGB'))
images.append(Image.open(r'/Users/usamamahmood/Developer/CISC442/PR3/Part2/images/persondoghorse.jpg').convert('RGB'))
images.append(Image.open(r'/Users/usamamahmood/Developer/CISC442/PR3/Part2/images/trainhorseperson.jpg').convert('RGB'))
images.append(Image.open(r'/Users/usamamahmood/Developer/CISC442/PR3/Part2/images/personbirdboat.jpg').convert('RGB'))





# files naems to save the result images
names = ['bikecarperson.jpg', 'cartrainplane.jpg', 'catdogperson.jpg', 'horsepersoncar.jpg', 'persondoghorse.jpg', 'trainhorseperson.jpg', 'personbirdboat.jpg']

# iterate through all images to segment objects and extract feature maps
name_i = 0 # iterate over names list to save the imagee with proper names
for i, image in enumerate(images):  
  print(f"processing {names[name_i]}")  
  transformed = transform(image)
  unsqueezed = torch.unsqueeze(transformed, 0)
  output = model(unsqueezed.to(device))
  out = output['out']
  #squeez the dimensions
  squeezed = out.squeeze()
  # extract segments
  segments = torch.argmax(squeezed, dim=0).cpu().numpy()
  # template to store the segmented image
  segmented_image = np.zeros_like(image).astype(np.uint8)

  # iterate over the class_map to color each segment
  for cl in range(1, len(class_map)):
      index = segments == cl
      segmented_image[index] = class_map[cl]
  

  # convert original image and segmented image into BGR channel
  original = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
  segmented_image = cv2.cvtColor(segmented_image, cv2.COLOR_RGB2BGR)

  # overlay segmented image on original image
  cv2.addWeighted(segmented_image, 0.6, original, 0.4, 0, original)
  #plt.imshow(original)
 
  # uncomment this line to save the result
  plt.imsave('/Users/usamamahmood/Developer/CISC442/PR3/Part2/Results/segmented/'+names[name_i], original)

  # Feature Extraction
  features = []
  # this nested loop will return 21 tiems to get 21 feature maps out of the segmented output.
  for i in range(3):
    for j in range(7):
    # i*7+j will give 7 features for every values of i
      feature = out[:, i*7+j,:,:].detach().cpu().numpy()
      feature = np.squeeze(feature)
      features.append(feature)
  '''
  Code for grids taken from https://matplotlib.org/stable/gallery/axes_grid1/simple_axesgrid.html
  '''
  tile_grid = np.zeros_like((3,7, ))
  fig = plt.figure(figsize=(15., 15.))
  grid = ImageGrid(fig, 111, # similar to subplot(111)
                  nrows_ncols=(3, 7),  # creates 2x2 grid of axes
                  axes_pad=0.1,  # pad between axes in inch.
                  )

  for ax, im in zip(grid, features):
      # Iterating over the grid returns the Axes.
      ax.imshow(im, cmap='gray')
      ax.set_axis_off()

  #plt.show()
  # uncomment this line to save the result
  fig.savefig('/Users/usamamahmood/Developer/CISC442/PR3/Part2/Results/features/'+names[name_i])
  name_i+=1